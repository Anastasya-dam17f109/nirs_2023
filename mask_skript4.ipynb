{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageOps,ImageStat,ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow  import keras as keras\n",
    "from keras import Model\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model # basic class for specifying and training a neural network\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten, Activation, Conv2D\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import  ModelCheckpoint\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import os,os.path\n",
    "import sys\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(173)\n",
    "#psp_fileName = \"D:/_SAR_Kubinka/__FRAGM1856X.JPG._PSP\"\n",
    "#image_fileName = \"D:/_SAR_Kubinka/__FRAGM1856X.JPG\"\n",
    "#newMask_fileName = \"D:/_SAR_Kubinka/new_mask_class_\"\n",
    "#fragm_dir = \"D:/_SAR_Kubinka/fragments\"\n",
    "#acc_file = 'D:/_SAR_Kubinka/accuracy.txt'\n",
    "#result_fileName = \"D:/_SAR_Kubinka/classification_result.BMP\"\n",
    "#marks_filename = 'D:/_SAR_Kubinka/class_marks.txt'\n",
    "#img_len = 1024\n",
    "\n",
    "img_file_name = image_fileName\n",
    "#дополнять массив цветами по мере необходимости\\n\",\n",
    "t_color = ['blue', 'red', 'yellow', 'green' , 'violet']\n",
    "temple_file = Image.open(img_file_name)\n",
    "colors = {}\n",
    "#сборка границ прямоугольникв, из которых будут вырезаться области (если их на изображении несколько)\n",
    "classRects=[]\n",
    "curColor = \"\"\n",
    "class_numb = '021'\n",
    "rect_flag  = False\n",
    "   \n",
    "img = Image.new('RGB', temple_file.size, color=0)\n",
    "draw  = ImageDraw.Draw(img)\n",
    "fp = open(psp_fileName)\n",
    "print(\"all was opened\")\n",
    "for k, txt in enumerate(fp):\n",
    "    if k<2:\n",
    "        continue\n",
    "    zz, t = txt.split('=')\n",
    "    if t.find('Pline') >= 0:\n",
    "        x2y = []\n",
    "        rect_flag = False\n",
    "        continue\n",
    "    if t.find('Rectangle') >= 0:\n",
    "        x2y = []\n",
    "        rect_flag = True\n",
    "        continue\n",
    "    if t.find('Pen') >= 0:\n",
    "        p = t.split(',')\n",
    "        curColor = p[2]\n",
    "        if p[2] not in  colors:\n",
    "            colors.update({p[2]: len(colors)})\n",
    "   \n",
    "    t = t.split(' ')\n",
    "    if t[0] == '':\n",
    "        if rect_flag:\n",
    "            x2y.insert(1,(x2y[0][0],x2y[1][1]))\n",
    "            x2y.append( (x2y[2][0], x2y[0][1]))  \n",
    "        idx = colors.get(curColor) \n",
    "        if idx ==  len(classRects):\n",
    "            classRects.append([x2y])\n",
    "        else:\n",
    "            classRects[idx].append(x2y)\n",
    "        #print(classRects)   \n",
    "        draw.polygon(x2y, outline = t_color[idx], fill = t_color[idx])\n",
    "        continue\n",
    "    x2y.append((int(t[0]), int(t[1])))\n",
    "print(\"masks were drawen\")\n",
    "img.save(newMask_fileName + class_numb +\".BMP\")\n",
    "img.close()\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = step_len * 2 + 1\n",
    "bound = math.ceil(math.sqrt(2)*win_size-win_size)\n",
    "dir_name = fragm_dir\n",
    "labels=[]\n",
    "nmb_fragm = 0\n",
    "print(bound)\n",
    "im = Image.open(img_file_name)\n",
    "for i in range(len(colors)):\n",
    "    for j in range(len(classRects[i])):\n",
    "        \n",
    "        x0 = classRects[i][j][0][0]\n",
    "        y0 = classRects[i][j][0][1]\n",
    "        x_len =  classRects[i][j][1][1]-classRects[i][j][0][1]\n",
    "        y_len =  classRects[i][j][2][0]-classRects[i][j][1][0]\n",
    "        x_amount = int(x_len/step_len)\n",
    "        y_amount = int(y_len/step_len)\n",
    "        for k in range(x_amount):\n",
    "            for l in range(y_amount):\n",
    "                labels.append(i)\n",
    "                im2=im.crop((x0 + k * step_len -3, y0+ l * step_len -3, x0 + k * step_len +3 + win_size,y0+ l * step_len +3 + win_size)) \n",
    "                imfnam=dir_name+\"\\\\+++fragm{:05d}.jpg\".format(nmb_fragm); \n",
    "                nmb_fragm+=1;\n",
    "                im2.save(imfnam)\n",
    "im.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transAmount = 6;\n",
    "dlin   = 0\n",
    "\n",
    "def load_CNN_train_augment(dir_name1):\n",
    "    ll=[]           #пустой список имен входных файлов JPG \n",
    "    for file in os.listdir(dir_name1):\n",
    "        if file.endswith(\".jpg\"): ll.append(file)\n",
    "    dlin   = len(ll)     \n",
    "    dlin0  = int(len(ll)/2)  #будем пропускать нечетные элементы\n",
    "    #dlin=int(dlin0*(var+1) )  #0 ->просто чтение, 1 ->+90, 2 ->+180, 3 ->+270 дополняем массив\n",
    "    train_x = np.zeros((dlin * transAmount, win_size, win_size, 3), dtype='float32')\n",
    "    print('train_x_augm.shape=',train_x.shape)\n",
    "    train_z = np.zeros((dlin * transAmount, 1), dtype='float32')\n",
    "    dlin = dlin * transAmount\n",
    "    k=0\n",
    "    for file in ll:\n",
    "        im  = Image.open(dir_name+\"\\\\\"+file)\n",
    "        train_x[transAmount * k] = np.array(im.crop((3,3,3 + win_size, 3 +win_size)))*1./255.    \n",
    "        train_x[transAmount * k + 1] = np.array(im.rotate(45).crop((3,3,3 + win_size,3 + win_size)))*1./255.    #print(b0.shape)\n",
    "        shift0 = random.randint(0,6)\n",
    "        shift1 = random.randint(0,6)\n",
    "        shift2 = random.randint(0,6)\n",
    "        shift3 = random.randint(0,6)\n",
    "        #shift0 = 3\n",
    "        #shift1 = 3\n",
    "        #shift2 = 3\n",
    "        #shift3 = 3\n",
    "        if shift0 == 0 and shift1 == 0:\n",
    "            shift0 += 2\n",
    "        if shift2 == 0 or shift2 == 3:    \n",
    "            shift2 +=1\n",
    "        if shift3 == 0 or shift3 == 3:    \n",
    "            shift3 +=1    \n",
    "        train_x[transAmount * k + 2] = np.array(im.crop((shift0,0,shift0 + win_size,win_size)))*1./255.    #print(b0.shape)\n",
    "        \n",
    "        train_x[transAmount * k + 3] = np.array(im.crop((0,shift1,win_size,shift1 + win_size)))*1./255.    #print(b0.shape)\n",
    "        train_x[transAmount * k + 4] = np.array(im.rotate(90).crop((shift2,shift3,shift2 + win_size,shift3 + win_size)))*1./255.\n",
    "        train_x[transAmount * k + 5] = np.array(im.rotate(180).crop((3,3,3+win_size,3+win_size)))*1./255.\n",
    "        #train_x[transAmount * k + 4] = np.array(im.crop((0,8,13,21)))*1./255.   \n",
    "        train_z[k * transAmount] = int(labels[k])\n",
    "        train_z[transAmount * k + 1] = int(labels[k])\n",
    "        train_z[transAmount * k + 3] = int(labels[k])\n",
    "        train_z[transAmount * k + 2] = int(labels[k])   \n",
    "        train_z[transAmount * k + 4] = int(labels[k])  \n",
    "        train_z[transAmount * k + 5] = int(labels[k])  \n",
    "        k+=1\n",
    "        if k==dlin: break\n",
    "    temp=[]\n",
    "    #for i in range(len(train_z)):\n",
    "       # temp.append(to_categorical(train_z[i],num_classes=len(colors)))\n",
    "    train_y=np.array(train_z)    \n",
    "    return train_x,train_y\n",
    "train_x1,train_y1=load_CNN_train_augment(dir_name)\n",
    "print(train_y1)\n",
    "'''\n",
    "train_x1,train_y1=load_CNN_train_augment(dir_name)\n",
    "%matplotlib inline\n",
    "def show_img_data(X_data,K0,rows,cols):   # смотрим на первые rows*cols \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4, 2), dpi=160) #, figsize=(20, 6)) \n",
    "    for j in range(rows):\n",
    "        for k in range(cols):\n",
    "            axes[j,k].set_axis_off()\n",
    "            axes[j,k].imshow(X_data[K0+j*cols+k].squeeze(), cmap='Greys', interpolation='None') #)'auto')\n",
    "\n",
    "show_img_data(train_x1,0,4,8) \n",
    "'''\n",
    "#np.random.shuffle(train_x1)\n",
    "#print('train_x_augm.shape=',train_x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_valid = 500\n",
    "k = 0\n",
    "idx_del = set()\n",
    "train_x_valid = np.zeros((amount_valid, win_size, win_size, 3), dtype='float32')\n",
    "train_y_valid = np.zeros((amount_valid,1,1), dtype='float32')\n",
    "while k < amount_valid:\n",
    "    idx = random.randint(0, train_x1.shape[0]-1)\n",
    "    while idx in idx_del:\n",
    "        idx = random.randint(0, train_x1.shape[0]-1)\n",
    "    idx_del.add(idx)\n",
    "    train_x_valid[k] = train_x1[idx]\n",
    "    train_y_valid[k] = train_y1[idx]\n",
    "    k += 1\n",
    "\n",
    "\n",
    "\n",
    "idx_del = list(idx_del)\n",
    "train_x_data = np.delete(train_x1, idx_del,0)\n",
    "train_y_data = np.delete(train_y1, idx_del,0)\n",
    "\n",
    "print('train_x_augm.shape=',train_y_valid.shape)\n",
    "print('train_x_augm.shape=',train_x_data.shape)\n",
    "#реорганизация набора тестовых данных, чтобы была равномерность \n",
    "train_x_valid_buf = np.zeros((train_x_data.shape[0]//2, win_size, win_size, 3), dtype='float32')\n",
    "train_y_valid_buf = np.zeros((train_x_data.shape[0]//2,1,1), dtype='float32')\n",
    "\n",
    "for i in range(train_x_data.shape[0]//2):\n",
    "    if i%2 ==0:\n",
    "        train_x_valid_buf[i] = train_x_data[2*i]\n",
    "        train_y_valid_buf[i] = train_y_data[2*i]\n",
    "np.flip( train_x_valid_buf)  \n",
    "np.flip( train_y_valid_buf)  \n",
    "for i in range(train_x_data.shape[0]//2):\n",
    "    if i%2 ==0:\n",
    "        train_x_data[2*i] = train_x_valid_buf[i]\n",
    "        train_y_data[2*i] = train_y_valid_buf[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train, height, width, depth = train_x_data.shape # there are 50000 training examples in CIFAR-10 \n",
    "print(num_train, height, width, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(len(colors), dtype='int')\n",
    "all_elems = np.zeros(len(colors), dtype='int')\n",
    "def estimateAccuracy(train_a,train_b,dlin_t,model_t):\n",
    "     for k in range(dlin_t):\n",
    "        ttt=np.zeros((1,win_size, win_size,3), dtype ='float32')\n",
    "        ttt[0]=train_a[k]\n",
    "        z = model_t.predict(ttt)   \n",
    "        tx = train_b[k,0]\n",
    "    \n",
    "        ty = z; #z[0]\n",
    "        m_idx = np.argmax(ty[0])  \n",
    "        real_idx = np.argmax(tx) \n",
    "        if  m_idx == real_idx:\n",
    "            accuracy[real_idx]  += 1\n",
    "            all_elems[real_idx] += 1\n",
    "        else:\n",
    "            all_elems[real_idx] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Предобработаем данные (это массивы Numpy)\n",
    "#x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "#x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "#y_train = y_train.astype('float32')\n",
    "#y_test = y_test.astype('float32')\n",
    "\n",
    "# Зарезервируем 10,000 примеров для валидации\n",
    "#x_val = x_train[-10000:]\n",
    "#y_val = y_train[-10000:]\n",
    "# = x_train[:-10000]\n",
    "#y_train = y_train[:-10000]\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "    #pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 3#height//2 # we will initially have 32 kernels per conv. layer...\n",
    "conv_depth_2 = height #64 # ...switching to 64 after the first pooling layer\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 68 # the FC layer will have 512 neurons\n",
    "num_classes=5\n",
    "inputs = keras.Input(shape=(11,11,), name='digits')\n",
    "#x = layers.Conv2D(conv_depth_1, (kernel_size, kernel_size), padding='same', activation='relu', input_shape = (height, width,depth)),(inputs)\n",
    "#x = layers.Conv2D(conv_depth_1, kernel_size, kernel_size, padding='same', activation='tanh')(x[1])\n",
    "#x = layers.MaxPooling2D(pool_size = (2, 2))(x[1])\n",
    "#x = layers.Flatten()(x)\n",
    "#outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "from tensorflow.keras.models import Sequential\n",
    "#model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model = Sequential()\n",
    "model.add(layers.Conv2D(conv_depth_1+3, kernel_size=3, padding='same', activation='relu', input_shape = (height, width,depth)))\n",
    "model.add(layers.Conv2D(5+1, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(layers.Conv2D(5+1, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(num_classes,  name='predictions'))\n",
    "#model.add(Dense(no_classes, activation='softmax'))\n",
    "# Создадим экземпляр оптимизатора.\n",
    "optimizer =  keras.optimizers.Adam(learning_rate=0.001)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "model.compile(loss='sparse_categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "# Reserve 10,000 samples for validation.\n",
    "#x_val = x_train[-10000:]\n",
    "#y_val = y_train[-10000:]\n",
    "#x_train = x_train[:-10000]\n",
    "#y_train = y_train[:-10000]\n",
    "\n",
    "# Подготовим тренировочный датасет.\n",
    "batch_size = 24\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices((train_x_data, train_y_data))\n",
    "#train_dataset = train_dataset.shuffle(buffer_size=12002).batch(batch_size)\n",
    "epochs = 45\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((train_x_valid, train_y_valid))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "data1 = np.zeros(epochs, dtype=float) \n",
    "data2 = np.zeros(epochs, dtype=float) \n",
    "data3 = np.zeros(epochs, dtype=float) \n",
    "data4 = np.zeros(epochs, dtype=float) \n",
    "data5 = np.zeros(epochs, dtype=float) \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x_data, train_y_data))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "# Итерируем по эпохам.\n",
    "val_acc_min=0.0\n",
    "\n",
    "gradhistory = []\n",
    "    \n",
    "def recordweight():\n",
    "    data = {}\n",
    "    for g,w in zip(grads, model.trainable_weights):\n",
    "        if '/kernel:' not in w.name:\n",
    "            continue # skip bias\n",
    "        name = w.name.split(\"/\")[0]\n",
    "        data[name] = g.numpy()\n",
    "    gradhistory.append(data)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    print('Начинаем эпоху %d' % (epoch,))\n",
    "   \n",
    "  # Итерируем по пакетам в датасете.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "    # Откроем GradientTape чтобы записать операции\n",
    "    # выполняемые во время прямого прохода, включающего автодифференцирование.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "      # Запустим прямой проход слоя.\n",
    "      # Операции применяемые слоем к своим\n",
    "      # входным данным будут записаны\n",
    "      # на GradientTape.\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "            #print(logits)\n",
    "           # print(y_batch_train)\n",
    "      # Вычислим значение потерь для этого минибатча.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "    # Используем gradient tape для автоматического извлечения градиентов\n",
    "    # обучаемых переменных относительно потерь.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        if step == 120:\n",
    "            print(len(model.trainable_weights))\n",
    "           # print(model.layers[0].trainable_weights)\n",
    "            #print(model.trainable_weights[0])\n",
    "            print(loss_value)\n",
    "            print(np.mean(abs(np.array(model.trainable_weights[0]))))\n",
    "            print(np.mean(abs(model.trainable_weights[1])))\n",
    "            print(np.mean(abs(model.trainable_weights[2])))\n",
    "            print(np.mean(abs(model.trainable_weights[3])))\n",
    "            print(np.mean(abs(model.trainable_weights[4])))\n",
    "            print(\"=============1==============\")\n",
    "            print(np.mean(abs(grads[1].numpy())), np.mean(abs(grads[1].numpy())/np.mean(abs(model.trainable_weights[0].numpy()))))\n",
    "            print(\"=============2==============\")\n",
    "            print(np.mean(abs(grads[2].numpy())), np.mean(abs(grads[2].numpy()))/np.mean(abs(model.trainable_weights[1].numpy())))\n",
    "            print(\"=============3==============\")\n",
    "            print(np.mean(abs(grads[3].numpy())), np.mean(abs(grads[3].numpy()))/np.mean(abs(model.trainable_weights[2].numpy())))\n",
    "            print(\"=============4==============\")\n",
    "            print(np.mean(abs(grads[4].numpy())), np.mean(abs(grads[4].numpy()))/np.mean(abs(model.trainable_weights[3].numpy())))\n",
    "            print(\"=============5==============\")\n",
    "            print(np.mean(abs(grads[5].numpy())), np.mean(abs(grads[5].numpy()))/np.mean(abs(model.trainable_weights[4].numpy())))\n",
    "            print(\"=============6==============\")\n",
    "            data1[epoch] = np.mean(abs(grads[1].numpy()))\n",
    "            data2[epoch] = np.mean(abs(grads[2].numpy()))\n",
    "            data3[epoch] = np.mean(abs(grads[3].numpy()))\n",
    "            data4[epoch] = np.mean(abs(grads[4].numpy()))\n",
    "            data5[epoch] = np.mean(abs(grads[5].numpy()))\n",
    "        #print(numpy.mean(abs(grads[6].numpy())))\n",
    "    # Выполним один шаг градиентного спуска обновив\n",
    "    # значение переменных минимизирующих потери.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "        if step == 0:\n",
    "            recordweight()\n",
    "    # Пишем лог каждые 200 пакетов.\n",
    "        if step % 200 == 0:\n",
    "        #print('Потери на обучении (для одного пакета) на шаге %s: %s' % (step, float(loss_value)))\n",
    "            print('Уже увидено: %s примеров' % ((step + 1) * 64))\n",
    "        #print(grads[].numpy())\n",
    "    recordweight()\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))    \n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = float(val_acc_metric.result())\n",
    "    if val_acc > val_acc_min:\n",
    "        model_temp = model\n",
    "        val_acc_min=val_acc\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    #print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, sharex=True, constrained_layout=True, figsize=(8, 12))\n",
    "ax[0].set_title(\"Mean gradient\")\n",
    "for key in gradhistory[0]:\n",
    "    ax[0].plot(range(len(gradhistory)), [w[key].mean() for w in gradhistory], label=key)\n",
    "ax[0].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(\"grad_shape = \", len(grads))\n",
    "import matplotlib.pyplot as plt\n",
    "gridsize = (1, 2)\n",
    "fig = plt.figure(figsize=(18, 4))\n",
    "ax1 = plt.subplot2grid(gridsize, (0, 0))\n",
    "ax2 = plt.subplot2grid(gridsize, (0, 1))\n",
    "#data = np.loadtxt(\"C:/Users/anastasya/Desktop/data_deviation.txt\").flatten()\n",
    "#print(data)\n",
    "#data = np.array([78.4,81.41,80.25,76.0,78.45], dtype = float)\n",
    "#ax1.set_title(title1 )\n",
    "#ax1.hist(data.flatten(), density=True)\n",
    "\n",
    "\n",
    "x = np.linspace(1, epochs, epochs)\n",
    "#ax1.stem(x,data-40)#ax1.spines[\"top\"].set_alpha(.3)\n",
    "ax1.grid()\n",
    "ax1.set_xlabel(\"Номер эпохи\")\n",
    "ax1.set_ylabel(\"Величина отклонения градиента\")\n",
    "\n",
    "ax1.plot(x, data1, color='red')\n",
    "ax1.plot(x, data3, color='blue')\n",
    "ax1.plot(x, data5, color='green')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temple_file = Image.open(img_file_name)\n",
    "img = Image.new('RGB', temple_file.size, color=0)\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "draw  = ImageDraw.Draw(img)\n",
    "width, height = img.size\n",
    "im2=np.zeros((1, win_size, win_size, 3), dtype='float32')\n",
    "class_marks = np.zeros((width,height), dtype='int')\n",
    "model = model_temp\n",
    "for i in range(0, width,step_len -1):\n",
    "    for j in range(0, height,step_len -1):\n",
    "        if(i + win_size) < width and (j + win_size ) < height:\n",
    "            x2y = [(i,j), (i + win_size, j),(i + win_size, j + win_size), (i, j + win_size)]\n",
    "            im2[0] = np.array(  temple_file.crop((i, j, i + win_size,j + win_size)))*1./255.\n",
    "            z = model.predict(im2)     \n",
    "            #print('z   ', z)\n",
    "            m_idx = np.argmax(z[0])\n",
    "            for k in range(win_size):\n",
    "                for l in range(win_size):\n",
    "                    class_marks[i + k, j + l] = m_idx #+ 1\n",
    "            draw.polygon(x2y, outline = t_color[m_idx], fill = t_color[m_idx])\n",
    "\n",
    "img.save(result_fileName)\n",
    "img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(marks_filename, 'w') as file:\n",
    "    for i in range(img_len):\n",
    "        for j in range(img_len):\n",
    "            file.write(str(class_marks[j, img_len-i] +1)) \n",
    "            file.write(\" \") \n",
    "        file.write(\"\\n\")\n",
    "    file.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsize = (1, 2)\n",
    "fig = plt.figure(figsize=(18, 16))\n",
    "ax1 = plt.subplot2grid(gridsize, (0, 0))\n",
    "ax2 = plt.subplot2grid(gridsize, (0, 1))\n",
    "data = np.loadtxt(marks_filename)\n",
    "pc = ax1.contourf(data)\n",
    "plt.colorbar(pc, ax=ax1, format='$%d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.run(['C:/Users/anastasya/CMakeBuilds/71f94088-de5b-a431-82e1-cdd7ec577f63/build/x64-Debug (по умолчанию)/exec/exec.exe',\"1\", metrics, marks_filename, results, faults])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
